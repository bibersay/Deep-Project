{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cycle GAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFLjubf+g3SIdMjTMXZxSv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t_5OXAmE3Iye"},"outputs":[],"source":["import glob\n","import random\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import sys\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import numpy as np\n","import math\n","import itertools\n","import datetime\n","import time\n","from torchvision.utils import save_image, make_grid\n","from torchvision import datasets\n","from torch.autograd import Variable"]},{"cell_type":"code","source":["!pip install kaggle\n","from google.colab import files, drive\n","\n","#구글 드라이브 마운트\n","drive.mount('/gdrive')\n","\n","#kaggle json 파일 가져오기\n","%cd /gdrive/MyDrive/kaggle\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","\n","# Permission Warning 방지\n","!chmod 600 ~/.kaggle/kaggle.json\n","%cd /content/\n","!kaggle datasets download -d arnaud58/selfie2anime  #kaggle API url 복사\n","\n","# 파일 압축 해제\n","!unzip /content/selfie2anime.zip"],"metadata":{"id":"_g9X3SgM8F32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # %cd /content/saved_model/\n","# !mkdir -p /gdrive/MyDrive/model\n","# !cp D_A_0.pth /gdrive/MyDrive/model\n","# !cp D_B_0.pth /gdrive/MyDrive/model\n","# !cp G_AB_0.pth /gdrive/MyDrive/model\n","# !cp G_BA_0.pth /gdrive/MyDrive/model\n","%cd /gdrive/MyDrive\n","!mkdir -p /content/saved_model/\n","!cp D_A_8.pth /content/saved_model/\n","!cp D_B_8.pth /content/saved_model/\n","!cp G_AB_8.pth /content/saved_model/\n","!cp G_BA_8.pth /content/saved_model/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OnRxodmZI6Ui","executionInfo":{"status":"ok","timestamp":1658900077311,"user_tz":-540,"elapsed":3214,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"2d141e28-5fab-44cd-93f9-9af55559f872"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive\n"]}]},{"cell_type":"code","source":["def to_rgb(image):\n","    rgb_image = Image.new(\"RGB\", image.size)\n","    rgb_image.paste(image)\n","    return rgb_image"],"metadata":{"id":"3yxzOBEJCVtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self,root, transforms_ = None, unaligned = False, mode = \"train\"):\n","        self.transform = transforms.Compose(transforms_)\n","        self.unaligned = unaligned\n","    \n","        if mode == \"train\":\n","            self.files_A = sorted(glob.glob(os.path.join(root, \"trainA\" + \"/*.*\")))\n","            self.files_B = sorted(glob.glob(os.path.join(root, \"trainB\" + \"/*.*\")))\n","            \n","        else :\n","            self.files_A = sorted(glob.glob(os.path.join(root, \"testA\" + \"/*.*\")))\n","            self.files_B = sorted(glob.glob(os.path.join(root, \"testB\" + \"/*.*\")))\n","            \n","    def __getitem__(self, index):\n","        image_A = Image.open(self.files_A[index % len(self.files_A)])\n","\n","        if self.unaligned:\n","            image_B = Image.open(self.files_B[random.randint(0,len(self.files_B) - 1)])        \n","        else :\n","            imgae_B = Image.open(self.files_B[index % len(self.files_B)])\n","\n","        if image_A.mode !=\"RGB\":\n","            image_A = to_rgb(image_A)\n","        \n","        if image_B.mode !=\"RGB\":\n","            image_B = to_rgb(image_B)\n","        \n","        item_A = self.transform(image_A)\n","        item_B = self.transform(image_B)\n","        return {\"A\": item_A, \"B\": item_B}\n","\n","    def __len__(self):\n","        return max(len(self.files_A), len(self.files_B))"],"metadata":{"id":"F5mf96wHJgTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def weights_init_normal(m):\n","    classname = m.__class__.__name__\n","    if classname.find(\"Conv\") != -1:\n","        torch.nn.init.normal_(m.weight.data, .0, .02)\n","        if hasattr(m, \"bias\") and m.bias is not None:\n","            torch.nn.init.constant_(m.bias.data, .0)\n","    \n","    elif classname.find(\"BatchNorm2d\") != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)"],"metadata":{"id":"5p-LROtIOO1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, in_features):\n","        super(ResidualBlock, self).__init__()\n","        self.block = nn.Sequential(\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_features, in_features, 3),\n","            nn.InstanceNorm2d(in_features),\n","            nn.ReLU(inplace = True),\n","            nn.ReflectionPad2d(1),\n","            nn.Conv2d(in_features, in_features, 3),\n","            nn.InstanceNorm2d(in_features),\n","        )\n","    def forward(self, x):\n","        return x + self.block(x)"],"metadata":{"id":"tr9-RJGmQ95n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GeneratorResNet(nn.Module):\n","    def __init__(self, input_shape, num_residual_blocks):\n","        super(GeneratorResNet, self).__init__()\n","        channels = input_shape[0]\n","\n","        out_features = 64\n","        model = [\n","                 nn.ReflectionPad2d(channels),\n","                 nn.Conv2d(channels, out_features, 7),\n","                 nn.InstanceNorm2d(out_features),\n","                 nn.ReLU(inplace = True),\n","        ]\n","\n","        in_features = out_features\n","\n","        for _ in range(2):\n","            out_features *=2\n","            model += [\n","                      nn.Conv2d(in_features, out_features, 3, stride = 2, padding =1),\n","                      nn.InstanceNorm2d(out_features),\n","                      nn.ReLU(inplace=True),\n","            ]\n","            in_features = out_features\n","\n","        for _ in range(num_residual_blocks):\n","            model += [ResidualBlock(out_features)]\n","\n","        for _ in range(2):\n","            out_features //=2\n","            model += [\n","            nn.Upsample(scale_factor =2),\n","            nn.Conv2d(in_features, out_features, 3, stride =1, padding =1),\n","            nn.InstanceNorm2d(out_features),\n","            nn.ReLU(inplace = True),\n","            ]\n","            in_features = out_features\n","\n","        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        return self.model(x)"],"metadata":{"id":"4n0zaMQlVYLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, input_shape):\n","        super(Discriminator, self).__init__()\n","        channel, height, width = input_shape\n","\n","        self.output_shape = (1, height//2 **4, width //2 **4)\n","\n","        def discriminator_block(in_filters, out_filters, normalize = True):\n","            layers = [nn.Conv2d(in_filters, out_filters, 4, stride =2, padding =1)]\n","\n","            if normalize:\n","                layers.append(nn.InstanceNorm2d(out_filters))\n","            layers.append(nn.LeakyReLU(.2, inplace=True))\n","            return layers\n","\n","        self.model = nn.Sequential(\n","            *discriminator_block(channel, 64, normalize =False),\n","            *discriminator_block(64,128),\n","            *discriminator_block(128,256),\n","            *discriminator_block(256,512),\n","            nn.ZeroPad2d((1,0,1,0)),\n","            nn.Conv2d(512,1,4,padding=1)\n","        )\n","\n","    def forward(self, img):\n","        return self.model(img)"],"metadata":{"id":"uGRMY84JZ3Oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_name = \"/content\"\n","channels = 3\n","img_height = 256\n","img_width = 256\n","n_residual_blocks = 9\n","lr=.0002\n","b1 = .5\n","b2 = .999\n","n_epochs = 200\n","init_epoch = 0\n","decay_epoch = 100\n","lambda_cyc = 100\n","lambda_id = 5.\n","n_cpu = 2\n","batch_size =1\n","sample_interval = 100\n","checkpoint_interval = 5"],"metadata":{"id":"pOdYpo33mcDE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.makedirs(\"/content/images/%s\" %dataset_name, exist_ok = True)\n","os.makedirs(\"/content/saved_model/%s\" %dataset_name, exist_ok = True)"],"metadata":{"id":"7i1keeytrLUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion_GAN = torch.nn.MSELoss()\n","criterion_cycle = torch.nn.L1Loss()\n","criterion_identity = torch.nn.L1Loss()"],"metadata":{"id":"3HDN9acmrVdI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_shape = (channels, img_height, img_width)\n","\n","G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n","G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n","D_A = Discriminator(input_shape)\n","D_B = Discriminator(input_shape)\n","\n","if os.path.isfile('/content/D_A_8.pth'):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # model.load_state_dict(torch.load(PATH, map_location=device))\n","    D_A.load_state_dict(torch.load('/content/D_A_8.pth', map_location=device))\n","    D_B.load_state_dict(torch.load('/content/D_B_8.pth', map_location=device))\n","\n","    G_AB.load_state_dict(torch.load('/content/G_AB_8.pth', map_location=device))\n","    G_BA.load_state_dict(torch.load('/content/G_BA_8.pth', map_location=device))\n","    # model.load_state_dict(torch.load(PATH))\n","else : \n","    G_AB.apply(weights_init_normal)\n","    G_BA.apply(weights_init_normal)\n","    D_A.apply(weights_init_normal)\n","    D_B.apply(weights_init_normal)"],"metadata":{"id":"iAPO8hvqr6PN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cuda = torch.cuda.is_available()\n","if cuda :\n","    G_AB = G_AB.cuda()\n","    G_BA = G_BA.cuda()\n","    D_A = D_A.cuda()\n","    D_B = D_B.cuda()\n","    criterion_GAN.cuda()\n","    criterion_cycle.cuda()\n","    criterion_identity.cuda()"],"metadata":{"id":"QtJFI_RmstqJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer_G = torch.optim.Adam(\n","    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",")\n","optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1,b2))\n","optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1,b2))"],"metadata":{"id":"uuiRq4aYw65M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LambdaLR:\n","    def __init__(self, n_epochs, offset, decay_start_epoch):\n","        assert (n_epochs - decay_start_epoch) > 0, \\\n","        \"Decay must start before the trainig session ends\"\n","        self.n_epochs = n_epochs\n","        self.offset = offset\n","        self.decay_start_epoch = decay_start_epoch\n","\n","    def step(self, epoch):\n","        return 1. - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)"],"metadata":{"id":"WGF2hdOXxPrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer_G, lr_lambda = LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",")\n","lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer_D_A, lr_lambda = LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",")\n","lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer_D_B, lr_lambda = LambdaLR(n_epochs, init_epoch, decay_epoch).step\n",")"],"metadata":{"id":"Rr4UWfWbx0wy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"],"metadata":{"id":"GJkcxQekyW6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer:\n","    def __init__(self,max_size=50):\n","        assert max_size >0 , \"Empty buffer or trying to create a black hole\"\n","\n","        self.max_size = max_size\n","        self.data = []\n","\n","    def push_and_pop(self, data):\n","        to_return = []\n","        for element in data.data:\n","            element = torch.unsqueeze(element,0)\n","            if len(self.data) < self.max_size :\n","                self.data.append(element)\n","                to_return.append(element)\n","            else:\n","                if random.uniform(0,1) > .5 :\n","                    i = random.randint(0,self.max_size -1)\n","                    to_return.append(self.data[i].clone())\n","                    self.data[i] = element\n","                else:\n","                    to_return.append(element)\n","        return Variable(torch.cat(to_return))"],"metadata":{"id":"Ma1UuDFNyapa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fake_A_buffer = ReplayBuffer()\n","fake_B_buffer = ReplayBuffer()"],"metadata":{"id":"zkz0As61d5KA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transforms_ = [\n","               transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n","               transforms.RandomCrop((img_height, img_width)),\n","               transforms.RandomHorizontalFlip(),\n","               transforms.ToTensor(),\n","               transforms.Normalize((.5,.5,.5), (.5,.5,.5)),\n","]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JuD7KVrd9cx","executionInfo":{"status":"ok","timestamp":1658900107922,"user_tz":-540,"elapsed":44,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"5e39525f-9f65-4a01-f766-1f674e1c6296"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n","  \"Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. \"\n"]}]},{"cell_type":"code","source":["dataloader = DataLoader(\n","    ImageDataset(dataset_name, transforms_ = transforms_, unaligned=True),\n","    batch_size = batch_size,\n","    shuffle = True,\n","    num_workers = n_cpu\n",")\n","\n","val_dataloader = DataLoader(\n","    ImageDataset(dataset_name, transforms_ = transforms_, unaligned=True, mode = \"test\"),\n","    batch_size = 5,\n","    shuffle = True,\n","    num_workers = 1\n",")"],"metadata":{"id":"Ksg4v5OBgKGi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_images(batches_done):\n","    imgs = next(iter(val_dataloader))\n","    G_AB.eval()\n","    G_BA.eval()\n","    real_A = Variable(imgs[\"A\"].type(Tensor))\n","    fake_B = G_AB(real_A)\n","    real_B = Variable(imgs[\"B\"].type(Tensor))\n","    fake_A = G_BA(real_B)\n","\n","    real_A = make_grid(real_A, nrow=5, normalize=True)\n","    real_B = make_grid(real_B, nrow=5, normalize=True)\n","    fake_A = make_grid(fake_A, nrow=5, normalize = True)\n","    fake_B = make_grid(fake_B, nrow=5, normalize = True)\n","\n","    image_grid = torch.cat((real_A, fake_B, real_B, fake_A),1)\n","    save_image(image_grid, \"%s/images/%s.png\" % (dataset_name, batches_done), normalize=False)"],"metadata":{"id":"pl_IOoCme9my"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prev_time = time.time()\n","for epoch in range(init_epoch, n_epochs):\n","    if epoch >=100:         # to test few epoches\n","        break\n","    for i, batch in enumerate(dataloader):\n","        if i>=40 :          # to test few batches\n","            break\n","        real_A = Variable(batch[\"A\"].type(Tensor))\n","        real_B = Variable(batch[\"B\"].type(Tensor))\n","\n","        valid = Variable(Tensor(np.ones((real_A.size(0),\n","                                         *D_A.output_shape))),\n","                         requires_grad = False)\n","        fake = Variable(Tensor(np.ones((real_A.size(0),\n","                                        *D_A.output_shape))),\n","                        requires_grad = False)\n","        \n","        G_AB.train()\n","        G_BA.train()\n","\n","        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n","        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n","        loss_identity = (loss_id_A + loss_id_B) /2\n","\n","        fake_B = G_AB(real_A)\n","        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n","        fake_A = G_BA(real_B)\n","        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n","        \n","        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n","\n","        recov_A = G_BA(fake_B)\n","        loss_cycle_A = criterion_cycle(recov_A, real_A)\n","\n","        recov_B = G_AB(fake_A)\n","        loss_cycle_B = criterion_cycle(recov_B, real_B)\n","        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n","\n","        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n","        loss_G.backward()\n","        optimizer_G.step()\n","        optimizer_D_A.zero_grad()\n","\n","\n","        loss_real = criterion_GAN(D_A(real_A), valid)\n","        fake_A = fake_A_buffer.push_and_pop(fake_A)\n","        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake)\n","        loss_D_A = (loss_real + loss_fake) /2\n","\n","        loss_D_A.backward()\n","        optimizer_D_A.step()\n","\n","        optimizer_D_B.zero_grad()\n","\n","        loss_real = criterion_GAN(D_B(real_B), valid)\n","        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n","        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n","\n","        loss_D_B = (loss_real + loss_fake) /2\n","        loss_D_B.backward()\n","        optimizer_D_B.step()\n","        loss_D = (loss_D_A + loss_D_B) /2\n","\n","        batches_done = epoch * len(dataloader) +i\n","        batches_left = n_epochs * len(dataloader) - batches_done\n","        time_left = datetime.timedelta(seconds= batches_left * (time.time() - prev_time))\n","        prev_time = time.time()\n","\n","        sys.stdout.write(\n","            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n","            % (\n","                epoch,\n","                n_epochs,\n","                i,\n","                len(dataloader),\n","                loss_D.item(),\n","                loss_G.item(),\n","                loss_GAN.item(),\n","                loss_cycle.item(),\n","                loss_identity.item(),\n","                time_left,\n","            )\n","        )\n","\n","        # (18) If at sample interval save image\n","        # if batches_done % sample_interval == 0:\n","        sample_images(batches_done)\n","\n","    # (19) Update learning rates\n","    lr_scheduler_G.step()\n","    lr_scheduler_D_A.step()\n","    lr_scheduler_D_B.step()\n","    # (20) Save model checkpoints\n","    if checkpoint_interval != -1 and epoch % 2 == 0:\n","        torch.save(G_AB.state_dict(), \"%s/saved_model/G_AB_%d.pth\" % (dataset_name, epoch))\n","        torch.save(G_BA.state_dict(), \"%s/saved_model/G_BA_%d.pth\" % (dataset_name, epoch))\n","        torch.save(D_A.state_dict(), \"%s/saved_model/D_A_%d.pth\" % (dataset_name, epoch))\n","        torch.save(D_B.state_dict(), \"%s/saved_model/D_B_%d.pth\" % (dataset_name, epoch))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"id":"-xr9rC2a3ST4","executionInfo":{"status":"error","timestamp":1658900838402,"user_tz":-540,"elapsed":730173,"user":{"displayName":"bong bong","userId":"17774416216702601338"}},"outputId":"078c09e0-5766-4b2c-ceca-57ef6ef22bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 0/200] [Batch 9/3400] [D loss: 0.816931] [G loss: 57.214153, adv: 0.913433, cycle: 0.536030, identity: 0.539553] ETA: 527 days, 23:13:16.997074"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-69d7de4f6a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_GAN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_cyc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_cycle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_id\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_identity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer_D_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}